{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of Contents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparation\n",
    "\n",
    "This section is where we prepare for the project, through a variety of initial steps. The steps in this section are as follows:\n",
    "\n",
    "- Importing Packages\n",
    "- Importing Data\n",
    "- Subsetting Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data management\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "#from pandas_profiling import ProfileReport\n",
    "\n",
    "#TextBlob Features\n",
    "from textblob import TextBlob\n",
    "\n",
    "#Plotting\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#SciKit-Learn\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#nltk\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "#nltk.download('stopwords')\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "#nltk.download('wordnet')\n",
    "\n",
    "#Tensorflow / Keras\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training Data\n",
    "path = \"../data/raw/twitter_training.csv\"\n",
    "train_df = pd.read_csv(path, names=[\"Tweet_ID\", \"Entity\", \"Sentiment\", \"Tweet_Content\"])\n",
    "\n",
    "#Test Data (Not to be used until the full model has been trained)\n",
    "test_path = \"../data/raw/twitter_validation.csv\"\n",
    "test_df = pd.read_csv(test_path, names=[\"Tweet_ID\", \"Entity\", \"Sentiment\", \"Tweet_Content\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subsetting Data\n",
    "\n",
    "As this dataset is quite large, during the exploration process we begin by subsetting the data during the training process, to speed up any testing steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = train_df.sample(frac=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Exploration\n",
    "\n",
    "Here, we explore the data, testing if it is balanced, and checking for patterns in missing rows. This can generally be done in an automated fashion with pandas-profiling. The sections under this header include:\n",
    "\n",
    "- Basic visualisation\n",
    "- Automated Data Exploration with pandas-profiling\n",
    "- Checking for balance in output categories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Will likely go under a header like \"data enrichment and editing\"\n",
    "- Creating Indexers for the Output Sentiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
